---
title: "Hierachial Clustering"
author: 
date: "2022-12-01"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
> ### Objectives
>
> -   k-means clustering - `kmeans()` function - Pros and cons
> -   Hierarchical clustering - `...` function - Different linkages (complete, single, average) - Make a dendrogram - Plotting Dendrogram using `dendextend`

------------------------------------------------------------------------

The data that we'll work with is the Wine Quality Data Set from the UCI Machine Learning Repository. It consists of 4898 observations of 12 attribute measures based on phsysiochemical tests:

1 - fixed acidity

2 - volatile acidity

3 - citric acid

4 - residual sugar

5 - chlorides

6 - free sulfur dioxide

7 - total sulfur dioxide

8 - density

9 - pH

10 - sulphates

11 - alcohol

There is one response variable based on sensory data: `quality`, which is measured by a score between 0 and 10.

# Hierarchical Clustering

In data mining and statistics, hierarchical clustering is a method of

cluster analysis that seeks to build a hierarchy of clusters.

##Step 1: Load the Necessary Packages

We will use the R package `cluster`, which provide methods for Cluster analysis, and the package `readr` to quickly read in our .csv file.

```{r}
require(readr) 
require(cluster)
require(factoextra)
```

##Step 2: Load and Prep the Data

At a closer glance, the observations in our raw data file, 'winequality-white.csv', are separated by semicolons. We load in our data file in the chunk below, and eliminate any `NA` values.

We also use the function `scale()`, which centers and scales the columns of a numeric matrix. By default, its arguments `center` and `scale` are both set to `TRUE`. If `center=TRUE`, centering is done by subtracting the column means of the data from their corresponding columns. If `scale=TRUE`, then scaling is done by dividing the (centered) columns of the data by their standard deviations (if `center =TRUE`). This results in each variable having a mean of 0 and standard deviation of 1.

```{r}
setwd("~/Desktop/PSTAT197A/GitHub/vignette-clustering/data")
wine <- read.csv('winequality-white.csv', sep=';') 
wine<-na.omit(wine) 
wine2 <- scale(wine) 
head(wine2) 
save(wine2, file = "clean-winequality.RData")
```


#Hierarchical clustering

Hierarchical clustering is an alternative to k-means clustering, a method of clustering that requires a pre-specified number of clusters $K$. Hierarchical clustering additionally produces a dendrogram, a tree-like representation of the similarity between observations,

Hierarchical clustering is an unsupervised learning technique to divide a dataset into clusters of observations, where each cluster contains observations that are 'close' to each other and clusters are 'far' from each other by a certain measure.

We will begin by using a sample 50 observations from the wine data set to reduce run time and make our visualizations (dendrograms) simpler.


```{r}
set.seed(100) 
w_ind <- sample(nrow(wine2),50,replace = F) 
winecut <- wine[w_ind,]
```

We will use the `agnes()` function from cluster package in R which computes agglomerative hierarchical clustering of the data. This function has two arguments, `data`, and `method`, which is a character string specifying the clustering method:

-   `"average"`: default method

-   `"single"`: single linkage

-   `"complete"`: complete linkage

-   `"ward"`: Ward's method

-   `"weighted"`: weighted average linkage and its generalization `"flexible"`

-   `"gaverage"`: a generalized `"average"`

Since we don't know beforehand which method will produce the best clusters, we can write a short function below to perform hierarchical clustering using several different methods. The function computes the agglomerative coefficient of each clustering method, which measures the strength of the clusters. The closer this value is to 1, the stronger the clusters are.

#Step 3: Find the Linkage Method to Use

We first define the linkage methods we will run through the function, then calculate agglomerative coefficient for each clustering linkage method in the chunk below.

```{r}
m <- c( "average", "single", "complete", "ward") 
names(m) <- c( "average", "single", "complete", "ward")
ac <- function(x) { agnes(winecut, method = x)$ac }
sapply(m, ac)
```

We can see that Ward's minimum variance method produces the highest agglomerative coefficient, thus we'll use that as the method for our final hierarchical clustering, which is performed in the chunk below.

```{r}
 clust <- agnes(winecut, method = "ward")
 pltree(clust, cex = 0.6, hang = -1, main = "Dendrogram")
```

We have so far only worked with the sample of 50 observations from the original `wine2` dataset. We now test various linkage methods on the whole `wine2` dataset in the chunk below and produce a dendrogram based on Ward's linkage method.

```{r}
m <- c( "average", "single", "complete", "ward") 
names(m) <- c( "average", "single", "complete", "ward") 
ac2 <- function(x) { agnes(wine2, method = x)$ac } 
sapply(m, ac2) 
clust2 <- agnes(wine2, method = "ward") 
pltree(clust2, cex = 0.6, hang = -1, main = "Dendrogram")
```


Each leaf at the bottom of the dendrogram represents an observation in the original dataset. As we move up the dendrogram from the bottom, observations that are similar to each other are fused together into a branch.

#Step 4: Determine the Optimal Number of Clusters We will be calculating the optimal number of clusters on the whole `wine2` dataset using the gap statistic, which compares the total intra-cluster variation for different values of $k$ with their expected values for a distribution with no clustering, to determine how many clusters the observations should be grouped in

We can calculate the gap statistic for each number of clusters using the `clusGap()` function from the cluster package along with a plot of Clusters vs. Gap Statistic using the `fviz_gap_stat()` function

We calculate the gap statistic for each number of clusters (up to 10 clusters) in the code below:

```{r}
gap_stat <- clusGap(wine2, FUN = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```


We observe that the gap statistic is highest at $k=3$ Thus, we select $k=3$ clusters to divide our dataset.

#Step 5: Apply Cluster Labels to Original Dataset To actually add cluster labels to each observation in our dataset, we can use the `cutree()` method to cut the dendrogram into 3 clusters:


```{r}
d <- dist(wine2, method = "euclidean")
final_clust <- hclust(d, method = "ward.D2" )
groups <- cutree(final_clust, k=3)
final_data <- cbind(wine, cluster = groups)
head(final_data)
```


# K-means Clustering

K-means clustering is an unsupervised learning method in machine learning (there is no response variable to evaluate the model off of). This method is used to group data points into K clusters where K is a tuning parameter chosen by the user. After initially randomly assigning data points to a cluster, the model iterates until the clusters are settled into their respective groups. The clustering is determined by the distance from the center of each cluster. The function used for k-means clustering is `kmeans()` found in the stats package.

```{r}
#REFERENCE: https://uc-r.github.io/kmeans_clustering
library(tidymodels)
library(cluster)
```

```{r}
#omit NA values, if there
wine2 <- na.omit(wine2)

#cutting down data for faster and clearer dendrogram
set.seed(100)
w_ind <- sample(nrow(wine2),50,replace = F)
winecut <- wine2[w_ind,]

#Standardize residuals to control variability, otherwise a feature with a large range will skew
wine_sc <- scale(winecut)

#Pick distance measure, here I use Euclidean from the factoextra R package
#install.packages("factoextra")
library(factoextra)
distance <- get_dist(wine_sc)
#fviz_dist allows us to visualize the distance
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"),show_labels = FALSE)
```

```{r}
#kmeans function used below. Here, we are setting centers = 2 to form two different clusters. Addiionally, the nstart option will attempt to create initial configurations in order to find the best one. In this instance, there are 30 configurations
kmeans2 <- kmeans(wine_sc, centers = 2, nstart = 30)
str(kmeans2)
```

```{r}
#Below you can see the two clusters, along with the cluster means
kmeans2
```

```{r}
#remember to remove labels
fviz_cluster(kmeans2, data = wine_sc)

```

```{r}
kmeans3 <- kmeans(wine_sc, centers = 3, nstart = 30)
kmeans4 <- kmeans(wine_sc, centers = 4, nstart = 30)
kmeans5 <- kmeans(wine_sc, centers = 5, nstart = 30)

# plots to compare
p1 <- fviz_cluster(kmeans2, geom = "point", data = wine_sc) + ggtitle("k = 2")
p2 <- fviz_cluster(kmeans3, geom = "point",  data = wine_sc) + ggtitle("k = 3")
p3 <- fviz_cluster(kmeans4, geom = "point",  data = wine_sc) + ggtitle("k = 4")
p4 <- fviz_cluster(kmeans5, geom = "point",  data = wine_sc) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

```{r}
#So, Which is best? There are three methods we can use 
#ELBOW
fviz_nbclust(wine_sc, kmeans, method = "wss")
#SILHOUETTE
fviz_nbclust(wine_sc, kmeans,method = "silhouette")
#GAP STATISTICS

gap_stat <- clusGap(wine_sc, FUN = kmeans, nstart = 30,
                    K.max = 10, B = 50)
print(gap_stat, method = "firstmax")
fviz_gap_stat(gap_stat)
#2 clusters seems to be the best!!! !
```

